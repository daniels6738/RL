Feito para a disciplina de Aprendizagem por Refor√ßo
Implementa√ß√£o de n-step TD-Learning off-policy para ambientes Gymnasium

<h1>Configura√ß√µes Iniciais</h1>

<h1>1 - TD Learning Off-Policy de n Passos</h1>
  O TD Learning Off-Policy de n passos √© uma extens√£o do TD Learning que utiliza uma sequ√™ncia de n passos para atualizar a estimativa do  Q(s,a) .
  
<h2>Relembrando o Off-Policy</h2>
  No aprendizado off-policy, a pol√≠tica de comportamento (que escolhe as a√ß√µes) √© diferente da pol√≠tica alvo (que √© avaliada). Um exemplo cl√°ssico √© o Q-Learning.
  
  <h2>TD Learning On-Policy</h2>
    No TD Learning on-policy, a mesma pol√≠tica √© usada tanto para escolher as a√ß√µes quanto para avaliar a pol√≠tica. Um exemplo √© o SARSA.
    
  <h2>Corre√ß√£o no TD Learning Off-Policy</h2>

No TD Learning off-policy, a estimativa do valor de uma pol√≠tica √© atualizada a partir de trajet√≥rias coletadas por outra pol√≠tica. Isso √© √∫til quando queremos aprender uma pol√≠tica √≥tima, mas os dados v√™m de uma pol√≠tica explorat√≥ria diferente (como um agente que tenta a√ß√µes aleat√≥rias para coletar mais informa√ß√µes sobre o ambiente).
<p>O problema principal √© que a distribui√ß√£o das a√ß√µes na pol√≠tica de comportamento (b) pode ser muito diferente da pol√≠tica alvo (ùúã), o que pode levar a atualiza√ß√µes enviesadas.</p> <p>Para corrigir essa discrep√¢ncia, utilizamos <b>Importance Sampling</b>, que ajusta a contribui√ß√£o das amostras para refletirem corretamente a pol√≠tica alvo.</p>
A chave para essa corre√ß√£o √© o <b>fator de import√¢ncia</b> (œÅ), definido como a raz√£o entre a probabilidade de uma sequ√™ncia de a√ß√µes sob a pol√≠tica alvo (œÄ) e sob a pol√≠tica de comportamento (b):

$œÅ_t = \frac{\pi(a_t|s_t)}{b(a_t|s_t)}$


Quando temos uma trajet√≥ria de n passos, o peso acumulado √© o produto dos fatores de import√¢ncia ao longo dos passos:


$œÅ = \prod_{t=1}^{n} \frac{\pi(a_t|s_t)}{b(a_t|s_t)}$


Esse fator √© fundamental porque ajusta a contribui√ß√£o das amostras geradas pela pol√≠tica de comportamento para refletirem corretamente a pol√≠tica alvo. Quando a pol√≠tica alvo e a de comportamento s√£o muito diferentes, os valores de œÅ podem variar drasticamente, tornando o aprendizado inst√°vel. O Weighted Importance Sampling resolve esse problema normalizando os pesos acumulados.


<h2>Exemplos Simples</h2>

Abaixo est√£o exemplos de como essa corre√ß√£o √© calculada para diferentes valores de n.

<h3>Ordinary Importance Sampling (n=1)</h3>

Para um epis√≥dio de um passo:
- Experi√™ncia: $( s, a, r_1, s_1, a_1 )$
- Estimativa: $Q_{target} = r_1 + \gamma Q(s_1, a_1) \cdot \frac{\pi(a_1|s_1)}{b(a_1|s_1)}$


<h3>Weighted Importance Sampling (n=2)</h3>

Para um epis√≥dio de dois passos:
- Experi√™ncia: $( s, a, r_1, s_1, a_1, r_2, s_2, a_2 )$
- Estimativa:
$Q_{target} = r_1 + \gamma r_2 + \gamma^2 Q(s_2, a_2) \cdot \frac{\pi(a_1|s_1) \pi(a_2|s_2)}{b(a_1|s_1) b(a_2|s_2)}$


O Weighted Importance Sampling normaliza esse peso para reduzir a variabilidade excessiva e melhorar a estabilidade do aprendizado.

<h2>Resumo</h2>
- <b>On-Policy (ex: SARSA)</b>: A pol√≠tica que escolhe as a√ß√µes √© a mesma que est√° sendo avaliada.<br>
- <b>Off-Policy (ex: Q-Learning, TD Off-Policy)</b>: A pol√≠tica que coleta experi√™ncias √© diferente da pol√≠tica alvo.<br>
- <b>Corre√ß√£o via Importance Sampling</b>: Ajusta as atualiza√ß√µes de Q para refletirem a pol√≠tica alvo corretamente.<br>
- <b>Rho (œÅ) √© o fator de ajuste</b>: Ele compensa as diferen√ßas entre as probabilidades das pol√≠ticas de comportamento e alvo.<br><br>

Esses conceitos garantem que o aprendizado ocorra de forma eficaz, mesmo quando a pol√≠tica de comportamento √© diferente da pol√≠tica alvo.




  <h2>O que foi implementado?</h2>
    Nesse projeto fizemos uso de Ordinary Importance Sampling para o aprendizado de ambientes discretos e cont√≠nuos em ambientes do Gymnasium.

  <h1>2 - Sobre os ambientes</h1>
    Escolhemos os ambientes discretos CliffWalking-v0, FrozenLake-v1 e Blackjack-v1; e escolhemos o ambiente cont√≠nuo Acrobot-v1.

  <h2>2.1 - CliffWalking</h2>
    <img  src="https://gymnasium.farama.org/_images/cliff_walking.gif" width="700px"/>
    <br>
    Um personagem come√ßa num local fixo e pode andar por um mapa, seu objetivo √© chegar at√© certo local fixo dando a menor quantidade de passos o poss√≠vel, "cair" em alguns espa√ßos espec√≠ficos, o personagem √© penalizado e retorna ao in√≠cio do caminho.

  <h2>2.2 - FrozenLake</h2>
    <img  src="https://gymnasium.farama.org/_images/frozen_lake.gif" width="300px"/>
    <br>
    Similarm ao CliffWalking, mas o personagem pode "escorregar", andando perpendicularmente √† dire√ß√£o desejada. No entanto, se o personagem cai em um dos buracos, o epis√≥dio termina.
    

  <h2>2.3 - Blackjack</h2>
    <img  src="https://gymnasium.farama.org/_images/blackjack.gif" 
    width="300px"/>
    <br>
    Similarmente ao blackjack real, as a√ß√µes s√£o escolher pegar mais uma carta ou parar, e o objetivo √© que o valor total de suas cartas seja mais pr√≥ximo de 21 que as cartas do Dealer, sem ultrapassar o valor.

  <h2>2.4 - Acrobot</h2>
    <img  src="https://gymnasium.farama.org/_images/acrobot.gif" width="300px"/>
    <br>
    No ambiente Acrobot, o espa√ßo de estados √© cont√≠nuo e as a√ß√µes t√™m espa√ßo discreto, representando a aplica√ß√£o de torque negativo, positivo, ou neutro nas juntas. O objetivo √© fazer com que a extremidade solta da barra ultrapasse a linha na menor quantidade de passos.


<h1>3 - Resultados</h1>
  <h2>Acrobot</h2>
  <img  src="https://github.com/user-attachments/assets/df02cc25-6af8-4a29-b28c-542e22aef2ff" width="700px"/>
  
  <h2>Blackjack</h2>
  <img  src="https://github.com/user-attachments/assets/ed443644-d3c0-43d6-8351-aedb4b6e8485" width="700px"/>

  <h2>CliffWalking</h2>
  <img  src="https://github.com/user-attachments/assets/d99d41cd-d6f8-412e-b5c3-7cdff5f676ea" width="700px" />
    
  <h2>FrozenLake</h2>
  <img  src="https://github.com/user-attachments/assets/fbfc36b3-836c-4343-9317-ce693a4e632c" width="700px"/>



<h1>Observa√ß√µes finais</h1>
 O TD Learning Off-Policy funciona bem em ambientes discretos e estruturados (como CliffWalking e Blackjack), mas pode ter dificuldades em cen√°rios com grandes penaliza√ß√µes e estrat√©gias de navega√ß√£o mais complexas. O uso de bootstrapping n-step permitiu um aprendizado r√°pido na maioria dos casos, mas o aprendizado demorou um pouco para come√ßar no Acrobot.<br><br>

 
 A estrat√©gia tamb√©m demonstrou instabilidade em alguns casos, tal como o FrozenLake, em que algumas execu√ß√µes o personagem raramente chegava ao objetivo.
Ajustes no Œµ-decay, nas recompensas e na taxa de aprendizagem podem melhorar o desempenho em futuros testes.
  

















      
